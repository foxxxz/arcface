{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "from arcface import calculate_arcface_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:(50000, 32, 32, 3), y_train:(50000,)\n",
      "x_test:(10000, 32, 32, 3), y_test:(10000,)\n",
      "y_train_ohe: (50000, 10)\n",
      "y_test_ohe: (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "ohe = OneHotEncoder()\n",
    "y_train_ohe = ohe.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "y_test_ohe = ohe.transform(y_test.reshape(-1,1)).toarray().astype('float32')\n",
    "print('x_train:{}, y_train:{}'.format(x_train.shape, y_train.shape))\n",
    "print('x_test:{}, y_test:{}'.format(x_test.shape, y_test.shape))\n",
    "print('y_train_ohe:', y_train_ohe.shape)\n",
    "print('y_test_ohe:', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['seed'] = 0\n",
    "params['embedding'] = 16\n",
    "params['n_classes'] = 10\n",
    "params['labels'] = np.unique(y_train).astype('int')\n",
    "params['batch_size'] = 128\n",
    "params['logits_scale'] = 10\n",
    "params['logits_margin'] = 0.1\n",
    "params['feed_limit'] = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Model = LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 18:04:16.949633 140602272642816 deprecation.py:323] From <ipython-input-4-3b35288236eb>:8: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0830 18:04:16.961263 140602272642816 deprecation.py:506] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0830 18:04:17.189656 140602272642816 deprecation.py:323] From <ipython-input-4-3b35288236eb>:10: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0830 18:04:17.332782 140602272642816 deprecation.py:323] From <ipython-input-4-3b35288236eb>:16: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0830 18:04:17.557295 140602272642816 deprecation.py:323] From <ipython-input-4-3b35288236eb>:18: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(params['seed'])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_ohe = tf.placeholder(tf.float32, [None, params['n_classes']])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=x, filters=6, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv1')\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "conv1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=conv1, filters=16, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv2')\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "conv2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "flatten = tf.layers.flatten(conv2)\n",
    "\n",
    "fc1 = tf.layers.dense(flatten, units=120, use_bias=True, activation=tf.nn.relu, name='fc1')\n",
    "fc2 = tf.layers.dense(fc1, units=84, use_bias=True, activation=tf.nn.relu, name='fc2')\n",
    "embedding_layer = tf.layers.dense(fc2, units=params['embedding'], use_bias=True, activation=None)\n",
    "\n",
    "clf_layer = tf.layers.dense(embedding_layer, units=params['n_classes'], activation=None, use_bias=False, name='clf_layer')\n",
    "\n",
    "with tf.variable_scope('clf_layer', reuse=True):\n",
    "    weights = tf.get_variable('kernel', trainable=True, regularizer=slim.l2_regularizer(1e-5))\n",
    "\n",
    "logits = calculate_arcface_logits(embedding_layer=embedding_layer, weights=weights, y=y_ohe,\n",
    "                                     class_num=params['n_classes'], s=params['logits_scale'], m=params['logits_margin'])\n",
    "softmax_layer = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1:(?, 14, 14, 6)\n",
      "conv2:(?, 5, 5, 16)\n",
      "fc1:(?, 120)\n",
      "fc2:(?, 84)\n",
      "embedding_layer:(?, 16)\n",
      "clf_layer:(?, 10)\n"
     ]
    }
   ],
   "source": [
    "print('conv1:{}'.format(conv1.shape))\n",
    "print('conv2:{}'.format(conv2.shape))\n",
    "print('fc1:{}'.format(fc1.shape))\n",
    "print('fc2:{}'.format(fc2.shape))\n",
    "print('embedding_layer:{}'.format(embedding_layer.shape))\n",
    "print('clf_layer:{}'.format(clf_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 18:04:18.046915 140602272642816 deprecation.py:323] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "eta = 1e-4\n",
    "epsilon = 1e-5\n",
    "loss = tf.reduce_sum(tf.maximum(tf.multiply(-tf.log(softmax_layer + epsilon), y_ohe), 0))\n",
    "train_model = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(centroid, sample):\n",
    "    inner_product = np.dot(sample, centroid)\n",
    "    centroid_dist = np.sqrt(np.sum(np.square(centroid)))\n",
    "    try:\n",
    "        sample_dist = np.sqrt(np.sum(np.square(sample), axis=1))\n",
    "    except:\n",
    "        sample_dist = np.sqrt(np.sum(np.square(sample)))\n",
    "    return 1 - (inner_product / (centroid_dist * sample_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1    13.39sec \n",
      "           train(loss:28703.4727, accuracy0.2640) \n",
      "           test (loss:28681.1152, accuracy0.2670)\n",
      "\n",
      "epoch:2    14.74sec \n",
      "           train(loss:24264.6719, accuracy0.4028) \n",
      "           test (loss:24416.9707, accuracy0.4026)\n",
      "\n",
      "epoch:3    13.20sec \n",
      "           train(loss:22335.3672, accuracy0.4520) \n",
      "           test (loss:22646.1953, accuracy0.4476)\n",
      "\n",
      "epoch:4    14.85sec \n",
      "           train(loss:21101.5820, accuracy0.4917) \n",
      "           test (loss:21523.3320, accuracy0.4772)\n",
      "\n",
      "epoch:5    13.21sec \n",
      "           train(loss:20124.7383, accuracy0.5194) \n",
      "           test (loss:20676.2070, accuracy0.4996)\n",
      "\n",
      "epoch:6    14.77sec \n",
      "           train(loss:19459.2129, accuracy0.5341) \n",
      "           test (loss:20076.4668, accuracy0.5111)\n",
      "\n",
      "epoch:7    13.21sec \n",
      "           train(loss:18921.8164, accuracy0.5472) \n",
      "           test (loss:19629.5781, accuracy0.5236)\n",
      "\n",
      "epoch:8    14.76sec \n",
      "           train(loss:18893.2812, accuracy0.5463) \n",
      "           test (loss:19648.8867, accuracy0.5200)\n",
      "\n",
      "epoch:9    13.29sec \n",
      "           train(loss:18065.2148, accuracy0.5689) \n",
      "           test (loss:18967.8418, accuracy0.5384)\n",
      "\n",
      "epoch:10    14.50sec \n",
      "           train(loss:17930.5938, accuracy0.5682) \n",
      "           test (loss:18916.6738, accuracy0.5373)\n",
      "\n",
      "epoch:11    13.28sec \n",
      "           train(loss:17560.3652, accuracy0.5798) \n",
      "           test (loss:18555.6523, accuracy0.5482)\n",
      "\n",
      "epoch:12    14.81sec \n",
      "           train(loss:17443.2422, accuracy0.5854) \n",
      "           test (loss:18484.4609, accuracy0.5529)\n",
      "\n",
      "epoch:13    13.47sec \n",
      "           train(loss:17437.0000, accuracy0.5842) \n",
      "           test (loss:18499.1328, accuracy0.5503)\n",
      "\n",
      "epoch:14    14.39sec \n",
      "           train(loss:16596.4453, accuracy0.6064) \n",
      "           test (loss:17907.6504, accuracy0.5636)\n",
      "\n",
      "epoch:15    13.49sec \n",
      "           train(loss:16605.0938, accuracy0.6011) \n",
      "           test (loss:17945.4844, accuracy0.5603)\n",
      "\n",
      "epoch:16    14.65sec \n",
      "           train(loss:16347.5996, accuracy0.6052) \n",
      "           test (loss:17865.6387, accuracy0.5516)\n",
      "\n",
      "epoch:17    13.14sec \n",
      "           train(loss:16292.1074, accuracy0.6151) \n",
      "           test (loss:17789.6152, accuracy0.5612)\n",
      "\n",
      "epoch:18    14.79sec \n",
      "           train(loss:16042.0879, accuracy0.6208) \n",
      "           test (loss:17705.6348, accuracy0.5670)\n",
      "\n",
      "epoch:19    13.45sec \n",
      "           train(loss:15704.0059, accuracy0.6272) \n",
      "           test (loss:17286.9941, accuracy0.5734)\n",
      "\n",
      "epoch:20    14.42sec \n",
      "           train(loss:15542.4395, accuracy0.6355) \n",
      "           test (loss:17305.8379, accuracy0.5751)\n",
      "\n",
      "epoch:21    13.39sec \n",
      "           train(loss:15355.5723, accuracy0.6321) \n",
      "           test (loss:17201.0215, accuracy0.5749)\n",
      "\n",
      "epoch:22    14.41sec \n",
      "           train(loss:15287.9824, accuracy0.6314) \n",
      "           test (loss:17240.5625, accuracy0.5742)\n",
      "\n",
      "epoch:23    13.53sec \n",
      "           train(loss:15077.2812, accuracy0.6431) \n",
      "           test (loss:17118.8359, accuracy0.5819)\n",
      "\n",
      "epoch:24    14.54sec \n",
      "           train(loss:14818.1436, accuracy0.6521) \n",
      "           test (loss:17044.0547, accuracy0.5822)\n",
      "\n",
      "epoch:25    13.48sec \n",
      "           train(loss:14751.3008, accuracy0.6531) \n",
      "           test (loss:17094.9082, accuracy0.5837)\n",
      "\n",
      "epoch:26    14.48sec \n",
      "           train(loss:14345.7402, accuracy0.6643) \n",
      "           test (loss:16884.6250, accuracy0.5875)\n",
      "\n",
      "epoch:27    13.52sec \n",
      "           train(loss:14182.5889, accuracy0.6711) \n",
      "           test (loss:16812.4375, accuracy0.5905)\n",
      "\n",
      "epoch:28    14.43sec \n",
      "           train(loss:14108.1035, accuracy0.6706) \n",
      "           test (loss:16915.9766, accuracy0.5899)\n",
      "\n",
      "epoch:29    13.39sec \n",
      "           train(loss:13924.2051, accuracy0.6751) \n",
      "           test (loss:16780.8730, accuracy0.5918)\n",
      "\n",
      "epoch:30    14.56sec \n",
      "           train(loss:14094.4658, accuracy0.6714) \n",
      "           test (loss:17101.0449, accuracy0.5906)\n",
      "\n",
      "epoch:31    13.56sec \n",
      "           train(loss:13507.9414, accuracy0.6935) \n",
      "           test (loss:16735.5586, accuracy0.5995)\n",
      "\n",
      "epoch:32    14.48sec \n",
      "           train(loss:13539.7148, accuracy0.6852) \n",
      "           test (loss:16799.5215, accuracy0.5979)\n",
      "\n",
      "epoch:33    13.47sec \n",
      "           train(loss:13767.3105, accuracy0.6737) \n",
      "           test (loss:17246.7812, accuracy0.5861)\n",
      "\n",
      "epoch:34    14.44sec \n",
      "           train(loss:13272.2676, accuracy0.6936) \n",
      "           test (loss:16884.6055, accuracy0.5948)\n",
      "\n",
      "epoch:35    13.61sec \n",
      "           train(loss:12889.7285, accuracy0.7011) \n",
      "           test (loss:16801.7129, accuracy0.5998)\n",
      "\n",
      "epoch:36    14.45sec \n",
      "           train(loss:12917.6895, accuracy0.7075) \n",
      "           test (loss:16863.9609, accuracy0.5975)\n",
      "\n",
      "epoch:37    13.65sec \n",
      "           train(loss:13386.5586, accuracy0.6880) \n",
      "           test (loss:17339.3477, accuracy0.5851)\n",
      "\n",
      "epoch:38    14.77sec \n",
      "           train(loss:12741.3584, accuracy0.7069) \n",
      "           test (loss:17103.1152, accuracy0.5976)\n",
      "\n",
      "epoch:39    13.68sec \n",
      "           train(loss:12426.7227, accuracy0.7154) \n",
      "           test (loss:16895.3438, accuracy0.5981)\n",
      "\n",
      "epoch:40    14.70sec \n",
      "           train(loss:12438.5039, accuracy0.7148) \n",
      "           test (loss:17032.9492, accuracy0.5987)\n",
      "\n",
      "epoch:41    13.75sec \n",
      "           train(loss:12344.9492, accuracy0.7173) \n",
      "           test (loss:17076.5195, accuracy0.5982)\n",
      "\n",
      "epoch:42    14.55sec \n",
      "           train(loss:12397.5352, accuracy0.7182) \n",
      "           test (loss:17378.3047, accuracy0.5942)\n",
      "\n",
      "epoch:43    13.82sec \n",
      "           train(loss:11868.3672, accuracy0.7320) \n",
      "           test (loss:17020.2422, accuracy0.5974)\n",
      "\n",
      "epoch:44    14.76sec \n",
      "           train(loss:11873.4814, accuracy0.7325) \n",
      "           test (loss:17076.8242, accuracy0.5975)\n",
      "\n",
      "epoch:45    13.74sec \n",
      "           train(loss:11793.0186, accuracy0.7334) \n",
      "           test (loss:17030.6836, accuracy0.6002)\n",
      "\n",
      "epoch:46    14.44sec \n",
      "           train(loss:11467.3887, accuracy0.7428) \n",
      "           test (loss:17052.7852, accuracy0.5998)\n",
      "\n",
      "epoch:47    13.76sec \n",
      "           train(loss:11328.7041, accuracy0.7428) \n",
      "           test (loss:17177.6074, accuracy0.5969)\n",
      "\n",
      "epoch:48    14.21sec \n",
      "           train(loss:11328.5898, accuracy0.7426) \n",
      "           test (loss:17306.4668, accuracy0.5984)\n",
      "\n",
      "epoch:49    14.05sec \n",
      "           train(loss:11585.2812, accuracy0.7360) \n",
      "           test (loss:17553.8555, accuracy0.5925)\n",
      "\n",
      "epoch:50    14.39sec \n",
      "           train(loss:11427.6201, accuracy0.7422) \n",
      "           test (loss:17675.4180, accuracy0.5935)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_path_train = []\n",
    "loss_path_test = []\n",
    "\n",
    "embedding_train = []\n",
    "embedding_test = []\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "weight_train = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "iter_cnt = 50\n",
    "np.random.seed(params['seed'])\n",
    "sample_size = 10000 # use 10000 samples only for monitoring\n",
    "\n",
    "# initial embedding train/test \n",
    "embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]})) \n",
    "\n",
    "# weight train/test\n",
    "weight_train.append(sess.run(weights))\n",
    "\n",
    "for epoch in range(iter_cnt):\n",
    "    start_time = time.time()\n",
    "    cursor = 0\n",
    "    step = 1\n",
    "    \n",
    "    # random shuffle\n",
    "    train_idx = np.arange(len(y_train))\n",
    "    np.random.shuffle(train_idx)\n",
    "    shuffled_x_train = x_train[train_idx]\n",
    "    shuffled_y_train_ohe = y_train_ohe[train_idx]\n",
    "\n",
    "    while cursor < len(y_train): \n",
    "        batch_x_train = shuffled_x_train[cursor:cursor+params['batch_size']]\n",
    "        batch_y_train_ohe = shuffled_y_train_ohe[cursor:cursor+params['batch_size']] \n",
    "        sess.run(train_model, feed_dict={x:batch_x_train, y_ohe:batch_y_train_ohe})       \n",
    "        step += 1\n",
    "        cursor += params['batch_size']\n",
    "    \n",
    "    # embedding train/test\n",
    "    embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "    embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]}))\n",
    "    \n",
    "    # loss train/test\n",
    "    loss_path_train.append(sess.run(loss, feed_dict={x:x_train[:sample_size], y_ohe:y_train_ohe[:sample_size]}))\n",
    "    loss_path_test.append(sess.run(loss, feed_dict={x:x_test[:sample_size], y_ohe:y_test_ohe[:sample_size]}))\n",
    "    \n",
    "    # weight\n",
    "    weight_train.append(sess.run(weights))\n",
    "    \n",
    "    # accuracy train/test\n",
    "    train_distance_list = []\n",
    "    test_distance_list = []\n",
    "    for c in weight_train[-1].T:\n",
    "        train_distance_list.append(cosine_distance(centroid=c, sample=embedding_train[-1][:sample_size]))\n",
    "        test_distance_list.append(cosine_distance(centroid=c, sample=embedding_test[-1][:sample_size]))\n",
    "    pred_train = pd.DataFrame(np.array(train_distance_list).T).idxmin(axis=1).values\n",
    "    pred_test = pd.DataFrame(np.array(test_distance_list).T).idxmin(axis=1).values\n",
    "    accuracy_train.append(np.sum((pred_train == y_train[:sample_size]).astype('int')) / len(y_train[:sample_size]))\n",
    "    accuracy_test.append(np.sum((pred_test == y_test[:sample_size]).astype('int')) / len(y_test[:sample_size]))\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('epoch:{}    {:.2f}sec \\n\\\n",
    "           train(loss:{:.4f}, accuracy{:.4f}) \\n\\\n",
    "           test (loss:{:.4f}, accuracy{:.4f})'.format(\n",
    "           epoch+1, end_time - start_time,\n",
    "           loss_path_train[-1], accuracy_train[-1],\n",
    "           loss_path_test[-1], accuracy_test[-1]))\n",
    "    print('')\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
